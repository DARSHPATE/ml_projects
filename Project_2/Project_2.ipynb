{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./homes_third1.csv\n",
      "./homes_third2.csv\n",
      "./homes.csv\n",
      "./Project_2.ipynb\n",
      "./homes_third3.csv\n",
      "./data_scraping.py\n",
      "./.ipynb_checkpoints/homes5-checkpoint.csv\n",
      "./.ipynb_checkpoints/homes2-checkpoint.csv\n",
      "./.ipynb_checkpoints/isntworking-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/homes-checkpoint.csv\n",
      "./.ipynb_checkpoints/Project_2_dupe_copy-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/Project_2-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/homes1-checkpoint.csv\n",
      "./.ipynb_checkpoints/homes_third2-checkpoint.csv\n",
      "./.ipynb_checkpoints/homes4-checkpoint.csv\n",
      "./.ipynb_checkpoints/data_scraping-checkpoint.py\n",
      "./.ipynb_checkpoints/homes_third3-checkpoint.csv\n",
      "./.ipynb_checkpoints/homes6-checkpoint.csv\n",
      "./.ipynb_checkpoints/homes_third1-checkpoint.csv\n",
      "./.ipynb_checkpoints/untitled-checkpoint.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"./\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technial but must be specific so the customer understands the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to use past housing data to predict the price of a home based on its attributes, limiting the analysis to a particular city. The city we are using for this is Pheonix, Arizona. We will use attributes from houses listed or sold up to at most two years ago, such as bedrooms, bathrooms, and square footage, as well as their prices, to construct a model that takes in those attributes and returns a predicted price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe following code was used to divide the data into 3 separate files so that it could be pushed to GitHub:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhomes.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mastype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m zero_rows \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[::\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      8\u001b[0m one_rows \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'astype'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The following code was used to divide the data into 3 separate files so that it could be pushed to GitHub:\n",
    "\n",
    "df = pd.read_csv('homes.csv', dtype=str)\n",
    "zero_rows = df.iloc[::3]\n",
    "one_rows = df.iloc[1::3]\n",
    "two_rows = df.iloc[2::3]\n",
    "zero_rows.to_csv('homes_third1.csv', index=False)\n",
    "one_rows.to_csv('homes_third2.csv', index=False)\n",
    "two_rows.to_csv('homes_third3.csv', index=False)\n",
    "    # For now, 'homes.csv' is too large to upload to git, so it is split into\n",
    "    # homes_third1.csv, homes_third2.csv, and homes_third3.csv so it can be pushed\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3729207/691811531.py:5: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv('homes_third1.csv')\n",
      "/tmp/ipykernel_3729207/691811531.py:7: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df3 = pd.read_csv('homes_third3.csv')\n",
      "/tmp/ipykernel_3729207/691811531.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, df1], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The following code can be used to reconstruct 'homes.csv' from 'homes_third1.csv', 'homes_third2.csv', and 'homes_third3.csv':\n",
    "\n",
    "df1 = pd.read_csv('homes_third1.csv', dtype=str)\n",
    "df2 = pd.read_csv('homes_third2.csv', dtype=str)\n",
    "df3 = pd.read_csv('homes_third3.csv', dtype=str)\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df = pd.concat([df, df3], ignore_index=True)\n",
    "df.iloc[::3] = df1\n",
    "df.iloc[1::3] = df2\n",
    "df.iloc[2::3] = df3\n",
    "df.to_csv('homes.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the HomeHarvest package to scrape housing data from realtor.com. Our parameters searched for houses sold and listed in the past 2 years, looping over every zip code in Pheonix; the code used to do this is located in data_scraping.py. Before any preprocessing, this means that some entries also include houses not in Pheonix that will likely need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explore = pd.read_csv('homes.csv')\n",
    "df_explore = df_explore[df_explore['status'] == 'SOLD']\n",
    "\n",
    "df_explore['sold_price'] = pd.to_numeric(df_explore['sold_price'], errors='coerce')\n",
    "df_explore['sqft'] = pd.to_numeric(df_explore['sqft'], errors='coerce')\n",
    "df_explore['year_built'] = pd.to_numeric(df_explore['year_built'], errors='coerce')\n",
    "df_explore['beds'] = pd.to_numeric(df_explore['beds'], errors='coerce')\n",
    "\n",
    "df_explore.dropna(subset=['sold_price'], inplace=True)\n",
    "df_explore.dropna(subset=['sqft'], inplace=True)\n",
    "df_explore.dropna(subset=['year_built'], inplace=True)\n",
    "df_explore.dropna(subset=['beds'], inplace=True)\n",
    "\n",
    "X_year = df_explore[['year_built']]\n",
    "X_sqft = df_explore[['sqft']]\n",
    "X_beds = df_explore[['beds']]\n",
    "Y = df_explore['sold_price']\n",
    "\n",
    "model_year = LinearRegression()\n",
    "model_year.fit(X_year, Y)\n",
    "model_sqft = LinearRegression()\n",
    "model_sqft.fit(X_sqft, Y)\n",
    "model_beds = LinearRegression()\n",
    "model_beds.fit(X_beds, Y)\n",
    "\n",
    "print('Attribute', 'Slope', 'Intercept', 'R squared')\n",
    "print('Year', model_year.coef_[0], model_year.intercept_, model_year.score(X_year, Y))\n",
    "print('Sqft', model_sqft.coef_[0], model_sqft.intercept_, model_sqft.score(X_sqft, Y))\n",
    "print('Beds', model_beds.coef_[0], model_beds.intercept_, model_beds.score(X_beds, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first note that we expect square footage, number of bedrooms, and number of bathrooms to all have a strong positive correlation with price, and we are biased towards considering these attributes in particular. To get a first look at the data, we picked a few attributes and performed a linear regression using that attribute against sell price for sold houses. We picked two of the attributes we suspected would have a strong positive correlation, square footage and bedrooms, as well as the year sold, for which we were unsure if there would be a strong correlation in either direction. The r^2 value for predicting sell price based off of year was 0.025, which is fairly weak, but strong enough that we will likely consider using it for our model. The r^2 values for square footage and bedrooms are 0.507 and 0.177 respectively, both of which indicate correlations of notable significance. The slopes of both of those regressions were also positive, as predicted.\n",
    "\n",
    "We should also note that these are all crude estimates, seeing as we only used sold houses, dropped NaNs from all attributes concurrently instead of individually, and have not performed the necessary preprocessing to restrict to houses in Pheonix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('homes.csv')\n",
    "df = df[df['city'] == 'Phoenix']\n",
    "    # The data includes some houses not in Pheonix; we wish to ignore those\n",
    "df = df[df['status'] == 'SOLD']\n",
    "    # To make it simpler, we will only look at houses that have been sold\n",
    "df.dropna(subset=['sold_price'], inplace=True) \n",
    "df = df[['sold_price', 'style', 'beds', 'full_baths', 'half_baths', 'sqft', 'year_built', 'stories', 'hoa_fee', 'parking_garage']]\n",
    "    # We include all data that we think will be both useful and usable\n",
    "    # For example, property_id is probably not useful,\n",
    "    # but text might be useful but is also hard to use\n",
    "df = pd.get_dummies(df, columns=['style'])\n",
    "    # We perform one hot encoding for style of home to use the categorical data\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    # Convert all data from strings to floats\n",
    "df['half_baths'] = df['half_baths'].fillna(0)\n",
    "    # Assume that a listing including no half-bathrooms implies that it has none\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best rusults possiable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer faceing Document provide summery of finding and detail approach taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self susficent and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infrence(prams):\n",
    "    results = m.run(prams)\n",
    "    return results "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
